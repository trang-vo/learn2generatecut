{"TOLERANCE": 1e-06, "CutEnv": {"usercut_type": "RewardShapingSubtourUserCallback", "features": ["support", "original", "static"], "node_dim": 1, "hidden_size": 64, "dropout": 0, "edge_dim": 1, "num_layers": 2, "ori_node_dim": 1, "ori_hidden_size": 64, "ori_dropout": 0, "ori_edge_dim": 4, "ori_num_layers": 2, "k": 10, "static_dim": 12, "limited_node": 2000, "n_clusters": 2, "reward": "gap", "penalty_no_cuts_time": 10, "penalty_no_cuts_num": 10, "penalty_no_cuts_gap": 10, "feature_extractor": "GNNFeatureExtractor"}, "CombineCutEnv": {"node_dim": 1, "edge_dim": 9, "static_dim": 11, "num_layers": 2, "hidden_size": 64, "dropout": 0, "limited_node": 2000, "n_clusters": 2, "reward": "gap", "penalty_no_cuts_time": 10, "penalty_no_cuts_num": 10, "penalty_no_cuts_gap": 50, "feature_extractor": "CombineFeatureExtractor"}, "RewardCombineCutEnv": {"node_dim": 1, "edge_dim": 9, "static_dim": 11, "num_layers": 2, "hidden_size": 64, "dropout": 0, "limited_node": 2000, "n_clusters": 2, "reward": "gap", "penalty_no_cuts_time": 10, "penalty_no_cuts_num": 10, "penalty_no_cuts_gap": 10, "feature_extractor": "CombineFeatureExtractor"}, "dqn": {"verbose": 1, "batch_size": 64, "target_update_interval": 3000, "train_freq": [512, "step"], "learning_starts": 0, "gamma": 1, "buffer_size": 512, "gradient_steps": 4, "pretrain_path": "", "exploration_initial_eps": 1, "exploration_final_eps": 0.05, "log_interval": 1}, "agent": {"total_timesteps": 1000000, "eval_freq": 2000, "n_eval_episodes": 1, "max_no_improvement_evals": 10000, "min_evals": 10000}, "gnn": {"node_dim": 1, "hidden_size": 64, "dropout": 0, "pretrain_path": "../models/GNN_mincut.pt"}, "rw_shaping": {"n_same_actions": 20, "pen_one_action": 1, "ncuts": 0.01}, "FromRootCutEnv": {"feature_extractor": "GNNFeatureExtractor", "features": ["support", "original", "static"], "node_dim": 1, "hidden_size": 64, "dropout": 0, "edge_dim": 1, "num_layers": 2, "ori_node_dim": 1, "ori_hidden_size": 64, "ori_dropout": 0.2, "ori_edge_dim": 4, "ori_num_layers": 2, "k": 10, "static_dim": 7, "limited_node": 2000, "n_clusters": 2, "reward": "time-gap", "penalty_no_cuts_time": 10, "penalty_no_cuts_num": 10, "penalty_no_cuts_gap": 10}}